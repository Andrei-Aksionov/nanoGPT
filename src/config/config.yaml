seed: 42

data:
  path:
    raw: data/raw
    interim: data/interim

datasets:
  tiny_shakespeare:
    url: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
    folder: ${data.path.raw}/tiny_shakespeare
    filename: input.txt
    file_path: ${datasets.tiny_shakespeare.folder}/${datasets.tiny_shakespeare.filename}

dataloader:
  test_split: 0.9
  num_workers: 4

model:
  checkpoint_folder: "models"
  tokenizer_folder: ${model.checkpoint_folder}/tokenizers
  bigram:
    batch_size: 32
    context_size: 8 # what is the maximum number of tokens to use during forward pass
    epochs: 1
    learning_rate: 1e-2
    checkpoint_model_path: ${model.checkpoint_folder}/bigram_model.pth.tar
    tokenizer_path: ${model.tokenizer_folder}/tokenizer_bigram.pkl
  gpt:
    size:
      large:
        embeddings_size: 384
        context_size: 256
        head_size: 32 # either should be provided explicitly or calculated implicitly as embeddings_size // num_heads
        num_heads: 6
        feed_forward_scaling: 4
        num_layers: 6
        dropout: 0.2
        batch_size: 64
        learning_rate: 3e-4
        epochs: 1
        checkpoint_model_path: ${model.checkpoint_folder}/gpt_model_large.pth.tar
        tokenizer_path: ${model.tokenizer_folder}/tokenizer_gpt_large.pkl
      small:
        embeddings_size: 64
        context_size: 8
        head_size: 8
        num_heads: 2
        feed_forward_scaling: 4
        num_layers: 2
        dropout: 0.2
        batch_size: 32
        learning_rate: 3e-4
        epochs: 1
        checkpoint_model_path: ${model.checkpoint_folder}/gpt_model_small.pth.tar
        tokenizer_path: ${model.tokenizer_folder}/tokenizer_gpt_small.pkl
