{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Example of training simple Bigram Language Model</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src import config\n",
    "from src.data.dataset import Dataset\n",
    "from src.data.tokenizer import CharTokenizer\n",
    "from src.model.bigram import BigramLanguageModel\n",
    "from src.model.trainer import Trainer\n",
    "from src.utils.data import train_test_split\n",
    "from src.utils.seed import set_seed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: load the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the simple model we will be using rather simple tiny shakespeare dataset, it consists of over 1 million of characters and the size is slightly over 1 Mb of disk space, so it's quite small. But the task for this repo is not to train the perfect language model for learning purposes, so this one should work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd().parents[1] / config.datasets.tiny_shakespeare.file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open(data_path, \"r\") as fin:\n",
    "    text = fin.read()\n",
    "\n",
    "print(text[:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the text consists of quote blocks with the name of the actor and his replica."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tokenize the text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model cannot work with characters, so we have to transform set of characters into a set of indices, where each index tells the position of the characters in the vocabulary.\n",
    "\n",
    "The input 'abc' will be transformed into [1, 2, 3], given that we have vocabulary {'a': 1, 'b': 2, 'c': 3}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing mapping of the 10 first characters.\n",
      "F -> 18\n",
      "i -> 47\n",
      "r -> 56\n",
      "s -> 57\n",
      "t -> 58\n",
      "  -> 1\n",
      "C -> 15\n",
      "i -> 47\n",
      "t -> 58\n",
      "i -> 47\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CharTokenizer(text)\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "\n",
    "print(\"Printing mapping of the 10 first characters.\")\n",
    "for idx in range(10):\n",
    "    print(f\"{text[idx]} -> {data[idx]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to split the data into two parts: train and test. The train part will be used during training, while test - during evaluation. Evaluation allows us to see how good the trained model predicts on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% for the training, 10% - fot the evaluating\n",
    "train_data, test_data = train_test_split(data, 0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader creates batches of tuples of the data, where the first element in the tuple is inputs, while the second - targets. Both are needed for the training and evaluating steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config.model.small\n",
    "block_size = model_config.block_size\n",
    "batch_size = model_config.batch_size\n",
    "\n",
    "# dataset class creates pairs (inputs, targets)\n",
    "train_dataset = Dataset(train_data, block_size)\n",
    "test_dataset = Dataset(test_data, block_size)\n",
    "\n",
    "# dataloader creates batches of pairs efficiently\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty straight forward: use trainer (contains logic for the training and evaluation) and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== Epoch: 0 ===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|##########| 3137/3137 [00:03<00:00, 880.94it/s, loss=2.4]  \n",
      "Evaluating: 100%|##########| 349/349 [00:04<00:00, 74.75it/s, loss=2.9] \n"
     ]
    }
   ],
   "source": [
    "set_seed(config.model.seed)\n",
    "\n",
    "model = BigramLanguageModel(vocab_size=tokenizer.vocab_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=model_config.learning_rate)\n",
    "trainer = Trainer(model, optimizer, train_dataloader, test_dataloader)\n",
    "trainer.train(epochs=config.model.small.epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate new characters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have trained model we can use it to create new characters. \n",
    "\n",
    "All we need is to provide context and the model will try to continue the text. If we provide tensor with zeros we basically do not proved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yotot cou oruld?\n",
      "NUS:\n",
      "OLiso umme isufr?\n",
      "SIUS:\n",
      "CI t whit Thin hed d fave GOLUTowe'shibroulengon athin\n"
     ]
    }
   ],
   "source": [
    "def generate_text(context: torch.Tensor) -> str:\n",
    "    return tokenizer.decode(model.generate(context, max_new_tokens=100).squeeze().tolist())\n",
    "\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(generate_text(context))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can provide first 10 characters as context, but in this case, for the simple bigram model, will not make any difference, as such model doesn't care about the context. But it will work with more advanced models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citils t s'twine alou CINIOM3Tatuliz\n",
      "\n",
      "As, d drvefep, hiat\n",
      "BR---whares or s s, g it,'d\n",
      "Shantee'de couthel\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor(tokenizer.encode(text[:10])).unsqueeze(dim=0)\n",
    "print(generate_text(context))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 28 2022, 17:36:10) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed6de188a6508f9f350a997ad6df62530f7a91cb3c4106b36ebaa288db005407"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
